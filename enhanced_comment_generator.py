import json
import logging
import os
from collections import Counter, defaultdict

import boto3
import pandas as pd

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger(__name__)

# ğŸ”‘ AWSãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãªã—ï¼‰
aws_profile = os.getenv("AWS_PROFILE")
if aws_profile:
    session = boto3.Session(profile_name=aws_profile)
else:
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼æƒ…å ±ã‚’ä½¿ç”¨
    session = boto3.Session()
s3 = session.client("s3")

# ğŸ“‚ S3ãƒã‚±ãƒƒãƒˆæƒ…å ±
BUCKET = "it-literacy-457604437098-ap-northeast-1"
PREFIX = "downloaded_jsonl_files_archive/"

# ğŸ“ å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
os.makedirs("output/analysis", exist_ok=True)


# ğŸ—“ ã‚«ãƒ†ã‚´ãƒªåˆ†é¡é–¢æ•°
def classify_category(yyyymm: str) -> str:
    month = int(yyyymm[4:])
    if month in [3, 4, 5]:
        return "æ˜¥"
    if month == 6:
        return "æ¢…é›¨"
    if month in [7, 8]:
        return "å¤"
    if month == 9:
        return "å°é¢¨"
    if month in [10, 11]:
        return "ç§‹"
    if month in [12, 1, 2]:
        return "å†¬"
    return "ä¸æ˜"


# ğŸ¯ å¤©å€™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå™¨
class WeatherPatternAnalyzer:
    def __init__(self):
        self.patterns = {
            "sunny": ["æ™´", "é™½", "æ—¥å·®ã—", "å¤ªé™½", "é’ç©º", "å¿«æ™´", "å¥½å¤©", "æ—¥å°„", "çœ©ã—", "ã¾ã¶ã—"],
            "cloudy": ["é›²", "æ›‡", "ã©ã‚“ã‚ˆã‚Š", "åšé›²", "è–„é›²", "é›²é–“", "é›²æµ·"],
            "rainy": ["é›¨", "å‚˜", "æ¿¡ã‚Œ", "æ¹¿", "ã˜ã‚ã˜ã‚", "ã—ã¨ã—ã¨", "ã‚¶ãƒ¼ã‚¶ãƒ¼", "ã½ã¤ã½ã¤", "é™æ°´", "ãƒ¬ã‚¤ãƒ³"],
            "stormy": ["é›·", "é›·é›¨", "çªé¢¨", "æš´é¢¨", "åµ", "è’å¤©", "å¼·é¢¨", "ã‚²ãƒªãƒ©", "ç«œå·»"],
            "mixed": ["å¤‰ã‚ã‚Š", "ç§»ã‚", "å¤‰åŒ–", "ä¸å®‰å®š", "ã“ã‚ã“ã‚", "ãŸã‚Š", "ä¸€æ™‚", "ã®ã¡"],
            "fog": ["éœ§", "ã‚‚ã‚„", "ã‹ã™ã¿", "éœ", "è¦–ç•Œ", "é„", "ãƒŸã‚¹ãƒˆ"],
            "snow": ["é›ª", "é›ªåŒ–ç²§", "ç²‰é›ª", "å¹é›ª", "é›ªé™", "ç©é›ª", "é›ªæ™¯è‰²"],
            "hot": ["æš‘", "çŒ›æš‘", "é…·æš‘", "ç‚å¤©", "ç†±ä¸­", "ç¼ç†±", "è’¸ã—æš‘", "çœŸå¤æ—¥"],
            "cold": ["å¯’", "å†·", "å‡", "æ°·", "éœœ", "æ¥µå¯’", "å³å¯’", "çœŸå†¬æ—¥"],
            "humid": ["æ¹¿", "ãƒ ã‚·ãƒ ã‚·", "ã˜ã‚ã˜ã‚", "ã¹ãŸã¹ãŸ", "è’¸ã—", "æ¹¿æ°—"],
            "dry": ["ä¹¾ç‡¥", "ã‚«ãƒ©ã‚«ãƒ©", "ãƒ‘ã‚µãƒ‘ã‚µ", "ä¹¾ã„", "æ¹¿åº¦ä½"],
            "wind": ["é¢¨", "ãã‚ˆé¢¨", "å¾®é¢¨", "å¼·é¢¨", "çªé¢¨", "ç„¡é¢¨", "é¢¨é€Ÿ", "ãƒ“ãƒ«é¢¨"],
            "special": ["é»„ç ‚", "èŠ±ç²‰", "ç´«å¤–ç·š", "UV", "ã‚ªã‚¾ãƒ³", "PM2.5", "å¤§æ°—"],
        }

    def analyze_comment(self, comment: str) -> dict[str, bool]:
        return {p: any(k in comment for k in ks) for p, ks in self.patterns.items()}

    def get_missing_patterns(self, top_comments: list[str]) -> list[str]:
        counts = defaultdict(int)
        for c in top_comments:
            for p, found in self.analyze_comment(c).items():
                if found:
                    counts[p] += 1
        return [p for p, c in counts.items() if c < 2]


# ğŸ“Š ã‚³ãƒ¡ãƒ³ãƒˆå“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
class CommentQualityScorer:
    def __init__(self):
        self.quality_indicators = {
            "å…·ä½“æ€§": ["å…·ä½“çš„", "è©³ç´°", "æ˜ç¢º", "ã¯ã£ãã‚Š", "ã—ã£ã‹ã‚Š"],
            "æ„Ÿæƒ…è¡¨ç¾": ["æ°—æŒã¡", "å¿«é©", "çˆ½ã‚„ã‹", "å¿ƒåœ°", "æ¥½ã—", "å¬‰ã—"],
            "è¡Œå‹•ææ¡ˆ": ["ãŠã™ã™ã‚", "æ³¨æ„", "æ°—ã‚’ã¤ã‘", "æº–å‚™", "å¯¾ç­–", "å·¥å¤«"],
            "æ™‚é–“æ€§": ["æœ", "æ˜¼", "å¤•", "å¤œ", "åˆå‰", "åˆå¾Œ", "æ˜ã‘æ–¹", "å¤•æ–¹"],
            "åœ°åŸŸæ€§": ["æµ·", "å±±", "éƒ½å¸‚", "éƒŠå¤–", "æ²¿å²¸", "å†…é™¸", "å¹³é‡", "ç›†åœ°"],
        }
        self.negative_indicators = ["ï¼Ÿï¼Ÿï¼Ÿ", "ä¸æ˜", "ã‚¨ãƒ©ãƒ¼", "###", "NULL", "none"]

    def score_comment(self, comment: str, count: int) -> float:
        if any(neg in comment for neg in self.negative_indicators):
            return 0.0
        base = min(10.0, count / 1000)
        bonus = sum(1.0 for ks in self.quality_indicators.values() if any(k in comment for k in ks))
        length_bonus = 2.0 if 4 <= len(comment) <= 12 else 1.0 if 3 <= len(comment) <= 15 else 0.0
        return base + bonus + length_bonus


# ğŸŒˆ ã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡ºå™¨
class SmartCommentExtractor:
    def __init__(self):
        self.analyzer = WeatherPatternAnalyzer()
        self.scorer = CommentQualityScorer()

    def extract_enhanced_comments(self, all_comments: list[str], current_top30: list[str], target_count: int = 100):
        current_set = set(current_top30)
        missing_patterns = self.analyzer.get_missing_patterns(current_top30)
        comment_counter = Counter(all_comments)
        candidates = []

        for comment, count in comment_counter.items():
            if comment in current_set:
                continue
            patterns = self.analyzer.analyze_comment(comment)
            score = self.scorer.score_comment(comment, count)
            missing_bonus = sum(5.0 for p in missing_patterns if patterns.get(p))
            total = score + missing_bonus
            candidates.append({"comment": comment, "count": count, "score": total, "patterns": patterns})

        candidates.sort(key=lambda x: x["score"], reverse=True)
        selected, pattern_counts = [], defaultdict(int)
        for c in candidates:
            if len(selected) >= target_count - 30:
                break
            if max((pattern_counts[p] for p, v in c["patterns"].items() if v), default=0) >= 3:
                continue
            selected.append(c)
            for p, v in c["patterns"].items():
                if v:
                    pattern_counts[p] += 1
        return selected


# ğŸ§  ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
def generate_analysis_report(category: str, top30: list[str], enhanced: list[dict], comment_type: str) -> str:
    analyzer = WeatherPatternAnalyzer()
    current = defaultdict(int)
    for c in top30:
        for p, found in analyzer.analyze_comment(c).items():
            if found:
                current[p] += 1

    new = defaultdict(int)
    for c in enhanced:
        for p, found in c["patterns"].items():
            if found:
                new[p] += 1

    report = f"""\n# {category}ã®{comment_type}åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n\n## ç¾åœ¨ã®TOP30ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒ\n"""
    for p, c in sorted(current.items()):
        report += f"- {p}: {c}ä»¶\n"

    report += """\n## è¿½åŠ æ¨å¥¨ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆä¾‹ï¼šä¸Šä½20ä»¶ / æ‹¡å¼µã‚³ãƒ¡ãƒ³ãƒˆå…¨ä½“ã§100ä»¶ï¼‰\n"""
    for i, c in enumerate(enhanced[:20], 1):
        ps = ", ".join([p for p, v in c["patterns"].items() if v])
        report += f"{i}. {c['comment']} (ä½¿ç”¨å›æ•°: {c['count']}, ã‚¹ã‚³ã‚¢: {c['score']:.1f})\n   ãƒ‘ã‚¿ãƒ¼ãƒ³: {ps}\n"

    report += """\n## è¿½åŠ å¾Œã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ”¹å–„\n"""
    for p in analyzer.patterns:
        c = current[p]
        n = new[p]
        if n > 0:
            report += f"- {p}: {c}ä»¶ â†’ {c + n}ä»¶ (+{n})\n"

    return report


# ğŸš€ ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    print("ğŸš€ ã‚¹ãƒãƒ¼ãƒˆã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡ºå™¨ã‚’é–‹å§‹...")
    response = s3.list_objects_v2(Bucket=BUCKET, Prefix=PREFIX)
    keys = [o["Key"] for o in response.get("Contents", []) if o["Key"].endswith(".jsonl")]

    weather_by_cat = defaultdict(list)
    advice_by_cat = defaultdict(list)

    for key in sorted(keys):
        yyyymm = key.split("/")[-1].replace(".jsonl", "")
        cat = classify_category(yyyymm)
        print(f"ğŸ“‚ {key} â†’ {cat}")
        obj = s3.get_object(Bucket=BUCKET, Key=key)
        lines = obj["Body"].read().decode("utf-8").splitlines()
        for line in lines:
            try:
                d = json.loads(line)
                wc = d.get("weather_comment", "").strip()
                adv = d.get("advice", "").strip()
                if wc:
                    weather_by_cat[cat].append(wc)
                if adv:
                    advice_by_cat[cat].append(adv)
            except json.JSONDecodeError as e:
                logger.warning(f"JSONDecodeError in line: {line[:50]}... Error: {e}")
                continue

    extractor = SmartCommentExtractor()

    for typ, dataset in [("weather_comment", weather_by_cat), ("advice", advice_by_cat)]:
        print(f"\nğŸ“ˆ {typ} ã®å‡¦ç†é–‹å§‹...")
        for cat, comments in dataset.items():
            if not comments:
                continue

            top30_tuples = Counter(comments).most_common(30)
            top30 = [c for c, _ in top30_tuples]
            enhanced = extractor.extract_enhanced_comments(comments, top30, target_count=100)

            df_top30 = pd.DataFrame(top30_tuples, columns=[typ, "count"])
            df_top30.to_csv(f"output/{cat}_{typ}_top30.csv", index=False, encoding="utf-8-sig")

            enhanced_tuples = [(c["comment"], c["count"]) for c in enhanced[:70]]
            all_combined = top30_tuples + enhanced_tuples
            df_all = pd.DataFrame(all_combined, columns=[typ, "count"])
            df_all.to_csv(f"output/{cat}_{typ}_enhanced100.csv", index=False, encoding="utf-8-sig")

            report = generate_analysis_report(cat, top30, enhanced, typ)
            with open(f"output/analysis/{cat}_{typ}_analysis.md", "w", encoding="utf-8") as f:
                f.write(report)

            print(f"âœ… {cat}: å‡ºåŠ›å®Œäº†ï¼ˆTop30 + Enhanced100ï¼‰")

    print("\nğŸ‰ å…¨å‡¦ç†å®Œäº†ï¼ output/ ã‚’ç¢ºèªã—ã¦ã­ï¼")


if __name__ == "__main__":
    main()
