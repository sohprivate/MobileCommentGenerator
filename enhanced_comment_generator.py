import json
import os
from collections import Counter, defaultdict
from typing import Dict, List, Set, Optional

import boto3
import pandas as pd

# ğŸ”‘ AWSãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: dit-trainingï¼‰
aws_profile = os.getenv("AWS_PROFILE", "dit-training")
session = boto3.Session(profile_name=aws_profile)
s3 = session.client("s3")

# ğŸ“‚ S3ãƒã‚±ãƒƒãƒˆæƒ…å ±
BUCKET = "it-literacy-457604437098-ap-northeast-1"
PREFIX = "downloaded_jsonl_files_archive/"

# ğŸ—ƒ å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
os.makedirs("output", exist_ok=True)


# ğŸ—“ ã‚«ãƒ†ã‚´ãƒªåˆ†é¡é–¢æ•°
def classify_category(yyyymm: str) -> str:
    month = int(yyyymm[4:])
    if month in [3, 4, 5]:
        return "æ˜¥"
    if month == 6:
        return "æ¢…é›¨"
    if month in [7, 8]:
        return "å¤"
    if month == 9:
        return "å°é¢¨"
    if month in [10, 11]:
        return "ç§‹"
    if month in [12, 1, 2]:
        return "å†¬"
    return "ä¸æ˜"


# ğŸ¯ å¤©å€™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå™¨
class WeatherPatternAnalyzer:
    def __init__(self):
        # å¤©å€™ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
        self.patterns = {
            "sunny": ["æ™´", "é™½", "æ—¥å·®ã—", "å¤ªé™½", "é’ç©º", "å¿«æ™´", "å¥½å¤©", "æ—¥å°„", "çœ©ã—", "ã¾ã¶ã—"],
            "cloudy": ["é›²", "æ›‡", "ã©ã‚“ã‚ˆã‚Š", "åšé›²", "è–„é›²", "é›²é–“", "é›²æµ·"],
            "rainy": ["é›¨", "å‚˜", "æ¿¡ã‚Œ", "æ¹¿", "ã˜ã‚ã˜ã‚", "ã—ã¨ã—ã¨", "ã‚¶ãƒ¼ã‚¶ãƒ¼", "ã½ã¤ã½ã¤", "é™æ°´", "ãƒ¬ã‚¤ãƒ³"],
            "stormy": ["é›·", "é›·é›¨", "çªé¢¨", "æš´é¢¨", "åµ", "è’å¤©", "å¼·é¢¨", "ã‚²ãƒªãƒ©", "ç«œå·»"],
            "mixed": ["å¤‰ã‚ã‚Š", "ç§»ã‚", "å¤‰åŒ–", "ä¸å®‰å®š", "ã“ã‚ã“ã‚", "ãŸã‚Š", "ä¸€æ™‚", "ã®ã¡"],
            "fog": ["éœ§", "ã‚‚ã‚„", "ã‹ã™ã¿", "éœ", "è¦–ç•Œ", "é„", "ãƒŸã‚¹ãƒˆ"],
            "snow": ["é›ª", "é›ªåŒ–ç²§", "ç²‰é›ª", "å¹é›ª", "é›ªé™", "ç©é›ª", "é›ªæ™¯è‰²"],
            "hot": ["æš‘", "çŒ›æš‘", "é…·æš‘", "ç‚å¤©", "ç†±ä¸­", "ç¼ç†±", "è’¸ã—æš‘", "çœŸå¤æ—¥"],
            "cold": ["å¯’", "å†·", "å‡", "æ°·", "éœœ", "æ¥µå¯’", "å³å¯’", "çœŸå†¬æ—¥"],
            "humid": ["æ¹¿", "ãƒ ã‚·ãƒ ã‚·", "ã˜ã‚ã˜ã‚", "ã¹ãŸã¹ãŸ", "è’¸ã—", "æ¹¿æ°—"],
            "dry": ["ä¹¾ç‡¥", "ã‚«ãƒ©ã‚«ãƒ©", "ãƒ‘ã‚µãƒ‘ã‚µ", "ä¹¾ã„", "æ¹¿åº¦ä½"],
            "wind": ["é¢¨", "ãã‚ˆé¢¨", "å¾®é¢¨", "å¼·é¢¨", "çªé¢¨", "ç„¡é¢¨", "é¢¨é€Ÿ", "ãƒ“ãƒ«é¢¨"],
            "special": ["é»„ç ‚", "èŠ±ç²‰", "ç´«å¤–ç·š", "UV", "ã‚ªã‚¾ãƒ³", "PM2.5", "å¤§æ°—"],
        }

    def analyze_comment(self, comment: str) -> Dict[str, bool]:
        """ã‚³ãƒ¡ãƒ³ãƒˆã®å¤©å€™ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        result = {}
        for pattern, keywords in self.patterns.items():
            result[pattern] = any(keyword in comment for keyword in keywords)
        return result

    def get_missing_patterns(self, current_top30: List[str]) -> List[str]:
        """ç¾åœ¨ã®TOP30ã§ä¸è¶³ã—ã¦ã„ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š"""
        pattern_counts = defaultdict(int)

        for comment in current_top30:
            analysis = self.analyze_comment(comment)
            for pattern, found in analysis.items():
                if found:
                    pattern_counts[pattern] += 1

        # ä¸è¶³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®šï¼ˆé–¾å€¤: 2ä»¶æœªæº€ï¼‰
        missing = []
        for pattern in self.patterns.keys():
            if pattern_counts[pattern] < 2:
                missing.append(pattern)

        return missing


# ğŸ” ã‚³ãƒ¡ãƒ³ãƒˆå“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
class CommentQualityScorer:
    def __init__(self):
        # é«˜å“è³ªæŒ‡æ¨™
        self.quality_indicators = {
            "å…·ä½“æ€§": ["å…·ä½“çš„", "è©³ç´°", "æ˜ç¢º", "ã¯ã£ãã‚Š", "ã—ã£ã‹ã‚Š"],
            "æ„Ÿæƒ…è¡¨ç¾": ["æ°—æŒã¡", "å¿«é©", "çˆ½ã‚„ã‹", "å¿ƒåœ°", "æ¥½ã—", "å¬‰ã—"],
            "è¡Œå‹•ææ¡ˆ": ["ãŠã™ã™ã‚", "æ³¨æ„", "æ°—ã‚’ã¤ã‘", "æº–å‚™", "å¯¾ç­–", "å·¥å¤«"],
            "æ™‚é–“æ€§": ["æœ", "æ˜¼", "å¤•", "å¤œ", "åˆå‰", "åˆå¾Œ", "æ˜ã‘æ–¹", "å¤•æ–¹"],
            "åœ°åŸŸæ€§": ["æµ·", "å±±", "éƒ½å¸‚", "éƒŠå¤–", "æ²¿å²¸", "å†…é™¸", "å¹³é‡", "ç›†åœ°"],
        }

        # ä½å“è³ªæŒ‡æ¨™ï¼ˆé™¤å¤–å¯¾è±¡ï¼‰
        self.negative_indicators = ["ï¼Ÿï¼Ÿï¼Ÿ", "ä¸æ˜", "ã‚¨ãƒ©ãƒ¼", "###", "NULL", "none"]

    def score_comment(self, comment: str, count: int) -> float:
        """ã‚³ãƒ¡ãƒ³ãƒˆã®å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        if any(neg in comment for neg in self.negative_indicators):
            return 0.0

        # åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆä½¿ç”¨å›æ•°ã®å¯¾æ•°ï¼‰
        base_score = min(10.0, count / 1000)

        # å“è³ªãƒœãƒ¼ãƒŠã‚¹
        quality_bonus = 0.0
        for category, indicators in self.quality_indicators.items():
            if any(ind in comment for ind in indicators):
                quality_bonus += 1.0

        # é•·ã•ãƒœãƒ¼ãƒŠã‚¹ï¼ˆé©åº¦ãªé•·ã•ã‚’è©•ä¾¡ï¼‰
        length_bonus = 0.0
        if 4 <= len(comment) <= 12:
            length_bonus = 2.0
        elif 3 <= len(comment) <= 15:
            length_bonus = 1.0

        return base_score + quality_bonus + length_bonus


# ğŸ¯ ã‚¹ãƒãƒ¼ãƒˆã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡ºå™¨
class SmartCommentExtractor:
    def __init__(self):
        self.analyzer = WeatherPatternAnalyzer()
        self.scorer = CommentQualityScorer()

    def extract_enhanced_comments(self, all_comments: List[str], current_top30: List[str], target_count: int = 50) -> List[tuple[str, int, Dict]]:
        """æ‹¡å¼µã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡º"""
        # ç¾åœ¨ã®TOP30ã‚’é™¤å¤–
        current_set = set(current_top30)

        # ä¸è¶³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š
        missing_patterns = self.analyzer.get_missing_patterns(current_top30)

        # å…¨ã‚³ãƒ¡ãƒ³ãƒˆã®ã‚«ã‚¦ãƒ³ãƒˆ
        comment_counter = Counter(all_comments)

        # å€™è£œã‚³ãƒ¡ãƒ³ãƒˆã‚’è©•ä¾¡
        candidates = []
        for comment, count in comment_counter.items():
            if comment in current_set:
                continue

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            pattern_analysis = self.analyzer.analyze_comment(comment)

            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            quality_score = self.scorer.score_comment(comment, count)

            # ä¸è¶³ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒœãƒ¼ãƒŠã‚¹
            missing_bonus = 0.0
            for pattern in missing_patterns:
                if pattern_analysis.get(pattern, False):
                    missing_bonus += 5.0

            total_score = quality_score + missing_bonus

            candidates.append(
                {
                    "comment": comment,
                    "count": count,
                    "quality_score": quality_score,
                    "missing_bonus": missing_bonus,
                    "total_score": total_score,
                    "patterns": pattern_analysis,
                }
            )

        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        candidates.sort(key=lambda x: x["total_score"], reverse=True)

        # å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ãŸé¸æŠ
        selected = []
        pattern_counts = defaultdict(int)

        for candidate in candidates:
            if len(selected) >= (target_count - 30):
                break

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒ©ãƒ³ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
            dominant_patterns = [p for p, found in candidate["patterns"].items() if found]

            # åŒä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³ã®éåº¦ãªé›†ä¸­ã‚’é¿ã‘ã‚‹
            if dominant_patterns:
                max_pattern_count = max(pattern_counts[p] for p in dominant_patterns) if dominant_patterns else 0
                if max_pattern_count >= 3:  # åŒä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³3ä»¶ã¾ã§
                    continue

            selected.append(candidate)
            for pattern in dominant_patterns:
                pattern_counts[pattern] += 1

        return selected


# ğŸ“Š åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
def generate_analysis_report(category: str, current_top30: List[str], enhanced_candidates: List[Dict], comment_type: str) -> str:
    """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    analyzer = WeatherPatternAnalyzer()

    # ç¾åœ¨ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    current_patterns = defaultdict(int)
    for comment in current_top30:
        analysis = analyzer.analyze_comment(comment)
        for pattern, found in analysis.items():
            if found:
                current_patterns[pattern] += 1

    # è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    new_patterns = defaultdict(int)
    for candidate in enhanced_candidates:
        for pattern, found in candidate["patterns"].items():
            if found:
                new_patterns[pattern] += 1

    report = f"""
# {category}ã®{comment_type}åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

## ç¾åœ¨ã®TOP30ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†å¸ƒ
"""
    for pattern, count in sorted(current_patterns.items()):
        report += f"- {pattern}: {count}ä»¶\n"

    report += """
## è¿½åŠ æ¨å¥¨ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆä¸Šä½10ä»¶ï¼‰
"""
    for i, candidate in enumerate(enhanced_candidates[:10], 1):
        patterns = [p for p, found in candidate["patterns"].items() if found]
        report += f"{i}. {candidate['comment']} (ä½¿ç”¨å›æ•°: {candidate['count']}, ã‚¹ã‚³ã‚¢: {candidate['total_score']:.1f})\n"
        report += f"   ãƒ‘ã‚¿ãƒ¼ãƒ³: {', '.join(patterns)}\n"

    report += """
## è¿½åŠ å¾Œã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ”¹å–„
"""
    for pattern in analyzer.patterns.keys():
        current = current_patterns[pattern]
        new = new_patterns[pattern]
        if new > 0:
            report += f"- {pattern}: {current}ä»¶ â†’ {current + new}ä»¶ (+{new})\n"

    return report


# ğŸ”„ ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    print("ğŸš€ ã‚¹ãƒãƒ¼ãƒˆã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡ºå™¨ã‚’é–‹å§‹...")

    # S3ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
    response = s3.list_objects_v2(Bucket=BUCKET, Prefix=PREFIX)
    jsonl_keys = [obj["Key"] for obj in response.get("Contents", []) if obj["Key"].endswith(".jsonl")]

    # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ã‚³ãƒ¡ãƒ³ãƒˆé›†è¨ˆ
    weather_by_category = defaultdict(list)
    advice_by_category = defaultdict(list)

    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    for key in sorted(jsonl_keys):
        yyyymm = key.split("/")[-1].replace(".jsonl", "")
        category = classify_category(yyyymm)
        print(f"ğŸ“‚ å‡¦ç†ä¸­: {key} â†’ ã‚«ãƒ†ã‚´ãƒª: {category}")

        obj = s3.get_object(Bucket=BUCKET, Key=key)
        lines = obj["Body"].read().decode("utf-8").splitlines()
        for line in lines:
            try:
                data = json.loads(line)
                wc = data.get("weather_comment")
                adv = data.get("advice")
                if wc and len(wc.strip()) > 0:
                    weather_by_category[category].append(wc.strip())
                if adv and len(adv.strip()) > 0:
                    advice_by_category[category].append(adv.strip())
            except json.JSONDecodeError:
                continue

    extractor = SmartCommentExtractor()

    # ğŸ“ˆ weather_comment ã®å‡¦ç†
    print("\nğŸŒ¤ Weather Commentå‡¦ç†é–‹å§‹...")
    for cat, comments in weather_by_category.items():
        if not comments:
            continue

        # ç¾åœ¨ã®TOP30
        current_top30_tuples = Counter(comments).most_common(30)
        current_top30 = [comment for comment, count in current_top30_tuples]

        # æ‹¡å¼µã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡º
        enhanced_candidates = extractor.extract_enhanced_comments(
            comments,
            current_top30,
            target_count=50,
        )

        # TOP30 CSVå‡ºåŠ›
        df_top30 = pd.DataFrame(current_top30_tuples, columns=["weather_comment", "count"])
        output_path_30 = f"output/{cat}_weather_comment_top30.csv"
        df_top30.to_csv(output_path_30, index=False)

        # æ‹¡å¼µç‰ˆCSVå‡ºåŠ›ï¼ˆTOP30 + è¿½åŠ 20ä»¶ï¼‰
        enhanced_tuples = [(c["comment"], c["count"]) for c in enhanced_candidates[:20]]
        all_enhanced = current_top30_tuples + enhanced_tuples
        df_enhanced = pd.DataFrame(all_enhanced, columns=["weather_comment", "count"])
        output_path_50 = f"output/{cat}_weather_comment_enhanced50.csv"
        df_enhanced.to_csv(output_path_50, index=False)

        # åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = generate_analysis_report(cat, current_top30, enhanced_candidates, "weather_comment")
        with open(f"output/analysis/{cat}_weather_analysis.md", "w", encoding="utf-8") as f:
            f.write(report)

        print(f"âœ… {cat}: TOP30={output_path_30}, Enhanced50={output_path_50}")

    # ğŸ“ˆ advice ã®å‡¦ç†
    print("\nğŸ’¡ Adviceå‡¦ç†é–‹å§‹...")
    for cat, advices in advice_by_category.items():
        if not advices:
            continue

        # ç¾åœ¨ã®TOP30
        current_top30_tuples = Counter(advices).most_common(30)
        current_top30 = [advice for advice, count in current_top30_tuples]

        # æ‹¡å¼µã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡º
        enhanced_candidates = extractor.extract_enhanced_comments(
            advices,
            current_top30,
            target_count=50,
        )

        # TOP30 CSVå‡ºåŠ›
        df_top30 = pd.DataFrame(current_top30_tuples, columns=["advice", "count"])
        output_path_30 = f"output/{cat}_advice_top30.csv"
        df_top30.to_csv(output_path_30, index=False)

        # æ‹¡å¼µç‰ˆCSVå‡ºåŠ›
        enhanced_tuples = [(c["comment"], c["count"]) for c in enhanced_candidates[:20]]
        all_enhanced = current_top30_tuples + enhanced_tuples
        df_enhanced = pd.DataFrame(all_enhanced, columns=["advice", "count"])
        output_path_50 = f"output/{cat}_advice_enhanced50.csv"
        df_enhanced.to_csv(output_path_50, index=False)

        # åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = generate_analysis_report(cat, current_top30, enhanced_candidates, "advice")
        with open(f"output/analysis/{cat}_advice_analysis.md", "w", encoding="utf-8") as f:
            f.write(report)

        print(f"âœ… {cat}: TOP30={output_path_30}, Enhanced50={output_path_50}")

    print("\nğŸ‰ ã‚¹ãƒãƒ¼ãƒˆã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡ºå®Œäº†ï¼")
    print("ğŸ“Š åˆ†æãƒ¬ãƒãƒ¼ãƒˆã¯ output/analysis/ ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç¢ºèªã—ã¦ãã ã•ã„")


if __name__ == "__main__":
    main()
